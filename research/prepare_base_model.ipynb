{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1458fc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\NARINDER\\\\Desktop\\\\Local Chatbot\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1c8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edca6d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\NARINDER\\\\Desktop\\\\Local Chatbot'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1234bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareBaseModelConfig:\n",
    "    root_path: Path\n",
    "    base_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    params_model_name: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.chatbot.constants import *\n",
    "from src.chatbot.utils.common import read_yaml, create_directory\n",
    "\n",
    "class ConfigurationManager:\n",
    "\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directory([self.config[self.config.artifact_root]])\n",
    "\n",
    "    def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "        config = self.config.prepare_base_model\n",
    "        create_directory([config.root_dir])\n",
    "        prepare_base_model_config = PrepareBaseModelConfig(\n",
    "            root_path = Path(config.root_dir),\n",
    "            base_model_path = Path(config.base_model_path),\n",
    "            updated_base_model_path = Path(config.updated_base_model_path),\n",
    "            params_model_name = self.params.model_name\n",
    "        )\n",
    "        return prepare_base_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6694b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "from typing import Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from llama_cpp import Llama\n",
    "from src.chatbot import logger\n",
    "\n",
    "class PrepareBaseModel:\n",
    "    \"\"\"\n",
    "    Straightforward preparer modeled after the provided Keras example.\n",
    "    - get_base_model: ensures `base_model_path` contains a HF model (copy or download).\n",
    "    - update_base_model: prepares and writes the updated model to `updated_base_model_path`.\n",
    "    - save_model: saves tokenizer and model in HF format.\n",
    "    \"\"\"\n",
    "\n",
    "    META_NAME = \"prepare_metadata.json\"\n",
    "\n",
    "    def __init__(self, config: PrepareBaseModelConfig):\n",
    "        self.config = config\n",
    "        self.base_model_path: Path = Path(config.base_model_path)\n",
    "        self.updated_base_model_path: Path = Path(config.updated_base_model_path)\n",
    "        self.params_model_name: str = config.params_model_name\n",
    "        Path(config.root_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _is_valid_model_dir(self, p: Path) -> bool:\n",
    "        if not p.exists() or not p.is_dir():\n",
    "            return False\n",
    "        if (p / \"config.json\").exists():\n",
    "            return True\n",
    "        if any(p.glob(\"pytorch_model*.bin\")) or any(p.glob(\"*.bin\")):\n",
    "            return True\n",
    "        if (p / \"tokenizer_config.json\").exists() or any(p.glob(\"tokenizer*\")) or any(p.glob(\"vocab*\")):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _write_metadata(self, source: str, status: str, extra: Optional[dict] = None):\n",
    "        meta = {\"source\": source, \"status\": status}\n",
    "        if extra:\n",
    "            meta.update(extra)\n",
    "        try:\n",
    "            self.updated_base_model_path.mkdir(parents=True, exist_ok=True)\n",
    "            with open(self.updated_base_model_path / self.META_NAME, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(meta, f, indent=2)\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to write metadata to %s\", self.updated_base_model_path)\n",
    "\n",
    "    def _copy_dir(self, src: Path, dst: Path):\n",
    "        if dst.exists():\n",
    "            shutil.rmtree(dst)\n",
    "        shutil.copytree(src, dst)\n",
    "\n",
    "    def _download_from_hub(self, model_name: str, dst: Path):\n",
    "        dst.mkdir(parents=True, exist_ok=True)\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        tok.save_pretrained(dst)\n",
    "        model.save_pretrained(dst)\n",
    "\n",
    "    def get_base_model(self) -> Path:\n",
    "        \"\"\"\n",
    "        Ensure there is a base model available at `self.base_model_path`.\n",
    "        Behavior:\n",
    "          - If base_model_path is valid, return it.\n",
    "          - Else if params_model_name provided, download model into base_model_path and return it.\n",
    "          - Else raise FileNotFoundError.\n",
    "        \"\"\"\n",
    "        # 1. If base exists and valid -> return\n",
    "        if self._is_valid_model_dir(self.base_model_path):\n",
    "            logger.info(\"Base model already present at %s\", self.base_model_path)\n",
    "            return self.base_model_path.resolve()\n",
    "\n",
    "        # 2. Try to download from hub if model name provided\n",
    "        if self.params_model_name:\n",
    "            logger.info(\"Downloading base model '%s' into %s\", self.params_model_name, self.base_model_path)\n",
    "            try:\n",
    "                # Ensure directory exists\n",
    "                self.base_model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Use llama_cpp to fetch and prepare the model\n",
    "                llm = Llama.from_pretrained(\n",
    "                    repo_id=self.params_model_name,\n",
    "                    filename=getattr(self, \"params_model_filename\", None),\n",
    "                    local_dir=str(self.base_model_path),\n",
    "                )\n",
    "\n",
    "                logger.info(\"Successfully loaded base model via llama_cpp into %s\", self.base_model_path)\n",
    "                return self.base_model_path.resolve()\n",
    "            except Exception:\n",
    "                logger.exception(\"Failed to download base model '%s'\", self.params_model_name)\n",
    "\n",
    "        # 3. Nothing worked -> error\n",
    "        msg = f\"No valid base model found at {self.base_model_path} and no hub model succeeded.\"\n",
    "        logger.error(msg)\n",
    "        raise FileNotFoundError(msg)\n",
    "\n",
    "    def update_base_model(self) -> Path:\n",
    "        \"\"\"\n",
    "        Ensure updated_base_model_path contains a usable model.\n",
    "        Behavior:\n",
    "          - If updated path already valid -> noop and return.\n",
    "          - Else, attempt to copy from base_model_path -> updated_base_model_path.\n",
    "          - If copy fails and params_model_name present, download to updated path.\n",
    "          - On success write metadata and return updated path.\n",
    "        \"\"\"\n",
    "        # If already valid, return early\n",
    "        if self._is_valid_model_dir(self.updated_base_model_path):\n",
    "            logger.info(\"Updated model already present at %s\", self.updated_base_model_path)\n",
    "            self._write_metadata(source=str(self.updated_base_model_path), status=\"already_present\")\n",
    "            return self.updated_base_model_path.resolve()\n",
    "\n",
    "        # Try copying from base_model_path\n",
    "        try:\n",
    "            base_path = self.get_base_model()  # will raise if not available\n",
    "            logger.info(\"Copying base model from %s to %s\", base_path, self.updated_base_model_path)\n",
    "            self._copy_dir(base_path, self.updated_base_model_path)\n",
    "            self._write_metadata(source=str(base_path), status=\"copied\")\n",
    "            return self.updated_base_model_path.resolve()\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Copy failed: %s\", e)\n",
    "\n",
    "        # Try downloading directly into updated path as fallback\n",
    "        if self.params_model_name:\n",
    "            try:\n",
    "                logger.info(\"Downloading model '%s' into %s as fallback\", self.params_model_name, self.updated_base_model_path)\n",
    "                self._download_from_hub(self.params_model_name, self.updated_base_model_path)\n",
    "                self._write_metadata(source=self.params_model_name, status=\"downloaded\")\n",
    "                return self.updated_base_model_path.resolve()\n",
    "            except Exception:\n",
    "                logger.exception(\"Failed to download model '%s' into %s\", self.params_model_name, self.updated_base_model_path)\n",
    "\n",
    "        # Final failure\n",
    "        msg = (\n",
    "            f\"Could not prepare updated base model. Checked copy from {self.base_model_path} \"\n",
    "            f\"and download of {self.params_model_name} into {self.updated_base_model_path}.\"\n",
    "        )\n",
    "        logger.error(msg)\n",
    "        raise FileNotFoundError(msg)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: AutoModelForCausalLM, tokenizer: AutoTokenizer):\n",
    "        \"\"\"\n",
    "        Save tokenizer and model in HF format to the given path.\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        model.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9a47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-21 11:59:03,071: INFO: common]: Reading YAML file from config\\config.yaml\n",
      "[2025-10-21 11:59:03,073: INFO: common]: Reading YAML file from params.yaml\n",
      "[2025-10-21 11:59:03,074: INFO: common]: Directory created at: artifacts\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "isinstance() arg 2 must be a type, a tuple of types, or a union",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     prepare_base_model.update_base_model()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     config = \u001b[43mConfigurationManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     prepare_base_model_config = config.get_prepare_base_model_config()\n\u001b[32m      4\u001b[39m     prepare_base_model = PrepareBaseModel(config= prepare_base_model_config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mConfigurationManager.__init__\u001b[39m\u001b[34m(self, config_filepath, params_filepath)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mself\u001b[39m.config = read_yaml(config_filepath)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.params = read_yaml(params_filepath)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mcreate_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43martifact_root\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NARINDER\\Desktop\\Local Chatbot\\venv\\Lib\\site-packages\\ensure\\main.py:873\u001b[39m, in \u001b[36mWrappedFunctionReturn.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    870\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EnsureError(msg.format(arg=arg, f=\u001b[38;5;28mself\u001b[39m.f, t=templ, valt=\u001b[38;5;28mtype\u001b[39m(value)))\n\u001b[32m    872\u001b[39m return_val = \u001b[38;5;28mself\u001b[39m.f(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreturn_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_templ\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    874\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mReturn value of \u001b[39m\u001b[38;5;132;01m{f}\u001b[39;00m\u001b[33m of type \u001b[39m\u001b[38;5;132;01m{valt}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mdoes not match annotation type \u001b[39m\u001b[38;5;132;01m{t}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    875\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EnsureError(msg.format(f=\u001b[38;5;28mself\u001b[39m.f, t=\u001b[38;5;28mself\u001b[39m.return_templ, valt=\u001b[38;5;28mtype\u001b[39m(return_val)))\n",
      "\u001b[31mTypeError\u001b[39m: isinstance() arg 2 must be a type, a tuple of types, or a union"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    prepare_base_model_config = config.get_prepare_base_model_config()\n",
    "    prepare_base_model = PrepareBaseModel(config= prepare_base_model_config)\n",
    "    prepare_base_model.get_base_model()\n",
    "    prepare_base_model.update_base_model()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4bf417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
